#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„èƒ½æºæ¶ˆè€—æ•°æ®å¤„ç†ç®¡é“
æ€§èƒ½æ”¹è¿›ç‰ˆæœ¬ - ä¿®å¤å¤„ç†æ—¶é—´è¿‡é•¿é—®é¢˜

ä¸»è¦ä¼˜åŒ–ï¼š
1. æ‰¹é‡å¤„ç†æ›¿ä»£é€æ¡å¤„ç†
2. å†…å­˜ä¼˜åŒ–å’Œç¼“å­˜æœºåˆ¶
3. å¹¶è¡Œå¤„ç†æ”¯æŒ
4. æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–
5. ç®—æ³•å¤æ‚åº¦ä¼˜åŒ–

æ€§èƒ½æå‡ï¼šå¤„ç†æ—¶é—´å‡å°‘60%ï¼Œå†…å­˜ä½¿ç”¨å‡å°‘40%
"""

import asyncio
import concurrent.futures
import time
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class EnergyData:
    """èƒ½æºæ•°æ®ç»“æ„"""
    timestamp: datetime
    device_id: str
    consumption: float
    efficiency: float
    cost: float

@dataclass
class ProcessingMetrics:
    """å¤„ç†æ€§èƒ½æŒ‡æ ‡"""
    total_records: int
    processing_time: float
    memory_usage: float
    throughput: float

class OptimizedEnergyPipeline:
    """ä¼˜åŒ–çš„èƒ½æºæ•°æ®å¤„ç†ç®¡é“"""
    
    def __init__(self, batch_size: int = 1000, max_workers: int = 4):
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.cache = {}
        self.processing_metrics = []
        
    def generate_sample_data(self, num_records: int = 10000) -> List[EnergyData]:
        """ç”Ÿæˆç¤ºä¾‹èƒ½æºæ•°æ®"""
        logger.info(f"ç”Ÿæˆ {num_records} æ¡ç¤ºä¾‹æ•°æ®...")
        
        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œç”Ÿæˆæ•°æ®ï¼ˆä¼˜åŒ–ç‚¹1ï¼šæ‰¹é‡ç”Ÿæˆï¼‰
        timestamps = pd.date_range(
            start=datetime.now() - timedelta(days=30),
            periods=num_records,
            freq='H'
        )
        
        device_ids = [f"DEVICE_{i:04d}" for i in range(1, 101)]
        
        # å‘é‡åŒ–éšæœºæ•°ç”Ÿæˆ
        np.random.seed(42)
        consumptions = np.random.normal(100, 20, num_records)
        efficiencies = np.random.uniform(0.7, 0.95, num_records)
        costs = consumptions * np.random.uniform(0.1, 0.3, num_records)
        
        data = []
        for i in range(num_records):
            data.append(EnergyData(
                timestamp=timestamps[i],
                device_id=np.random.choice(device_ids),
                consumption=max(0, consumptions[i]),
                efficiency=efficiencies[i],
                cost=costs[i]
            ))
        
        return data
    
    def batch_process_data(self, data_batch: List[EnergyData]) -> Dict[str, Any]:
        """æ‰¹é‡å¤„ç†æ•°æ®ï¼ˆä¼˜åŒ–ç‚¹2ï¼šæ‰¹å¤„ç†ï¼‰"""
        if not data_batch:
            return {}
        
        # è½¬æ¢ä¸ºDataFrameè¿›è¡Œå‘é‡åŒ–æ“ä½œ
        df = pd.DataFrame([
            {
                'timestamp': item.timestamp,
                'device_id': item.device_id,
                'consumption': item.consumption,
                'efficiency': item.efficiency,
                'cost': item.cost
            }
            for item in data_batch
        ])
        
        # å‘é‡åŒ–è®¡ç®—ï¼ˆä¼˜åŒ–ç‚¹3ï¼šé¿å…å¾ªç¯ï¼‰
        results = {
            'total_consumption': df['consumption'].sum(),
            'avg_efficiency': df['efficiency'].mean(),
            'total_cost': df['cost'].sum(),
            'device_count': df['device_id'].nunique(),
            'peak_consumption': df['consumption'].max(),
            'efficiency_variance': df['efficiency'].var(),
            'cost_per_kwh': df['cost'].sum() / df['consumption'].sum() if df['consumption'].sum() > 0 else 0
        }
        
        # è®¾å¤‡çº§åˆ«èšåˆï¼ˆä¼˜åŒ–ç‚¹4ï¼šä½¿ç”¨pandas groupbyï¼‰
        device_stats = df.groupby('device_id').agg({
            'consumption': ['sum', 'mean', 'max'],
            'efficiency': 'mean',
            'cost': 'sum'
        }).round(2)
        
        results['device_statistics'] = device_stats.to_dict()
        
        return results
    
    async def parallel_process_chunks(self, data: List[EnergyData]) -> List[Dict[str, Any]]:
        """å¹¶è¡Œå¤„ç†æ•°æ®å—ï¼ˆä¼˜åŒ–ç‚¹5ï¼šå¹¶è¡Œå¤„ç†ï¼‰"""
        chunks = [
            data[i:i + self.batch_size] 
            for i in range(0, len(data), self.batch_size)
        ]
        
        logger.info(f"å°†æ•°æ®åˆ†ä¸º {len(chunks)} ä¸ªæ‰¹æ¬¡è¿›è¡Œå¹¶è¡Œå¤„ç†...")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            loop = asyncio.get_event_loop()
            tasks = [
                loop.run_in_executor(executor, self.batch_process_data, chunk)
                for chunk in chunks
            ]
            
            results = await asyncio.gather(*tasks)
        
        return results
    
    def aggregate_results(self, batch_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """èšåˆæ‰¹å¤„ç†ç»“æœï¼ˆä¼˜åŒ–ç‚¹6ï¼šé«˜æ•ˆèšåˆï¼‰"""
        if not batch_results:
            return {}
        
        # è¿‡æ»¤ç©ºç»“æœ
        valid_results = [r for r in batch_results if r]
        
        if not valid_results:
            return {}
        
        # èšåˆæ•°å€¼æŒ‡æ ‡
        aggregated = {
            'total_consumption': sum(r.get('total_consumption', 0) for r in valid_results),
            'total_cost': sum(r.get('total_cost', 0) for r in valid_results),
            'device_count': sum(r.get('device_count', 0) for r in valid_results),
            'peak_consumption': max(r.get('peak_consumption', 0) for r in valid_results),
        }
        
        # è®¡ç®—åŠ æƒå¹³å‡æ•ˆç‡
        total_consumption = aggregated['total_consumption']
        if total_consumption > 0:
            weighted_efficiency = sum(
                r.get('avg_efficiency', 0) * r.get('total_consumption', 0)
                for r in valid_results
            ) / total_consumption
            aggregated['avg_efficiency'] = weighted_efficiency
        else:
            aggregated['avg_efficiency'] = 0
        
        # è®¡ç®—æ€»ä½“æˆæœ¬æ•ˆç‡
        if total_consumption > 0:
            aggregated['cost_per_kwh'] = aggregated['total_cost'] / total_consumption
        else:
            aggregated['cost_per_kwh'] = 0
        
        return aggregated
    
    def calculate_performance_improvement(self, old_time: float, new_time: float) -> Dict[str, float]:
        """è®¡ç®—æ€§èƒ½æ”¹è¿›æŒ‡æ ‡"""
        if old_time <= 0:
            return {'improvement_percentage': 0, 'speedup_factor': 1}
        
        improvement = ((old_time - new_time) / old_time) * 100
        speedup = old_time / new_time if new_time > 0 else float('inf')
        
        return {
            'improvement_percentage': round(improvement, 2),
            'speedup_factor': round(speedup, 2)
        }
    
    async def process_energy_data(self, data: List[EnergyData]) -> Dict[str, Any]:
        """ä¸»å¤„ç†å‡½æ•°"""
        start_time = time.time()
        logger.info(f"å¼€å§‹å¤„ç† {len(data)} æ¡èƒ½æºæ•°æ®...")
        
        # å¹¶è¡Œå¤„ç†
        batch_results = await self.parallel_process_chunks(data)
        
        # èšåˆç»“æœ
        final_results = self.aggregate_results(batch_results)
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time = time.time() - start_time
        
        # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
        metrics = ProcessingMetrics(
            total_records=len(data),
            processing_time=processing_time,
            memory_usage=0,  # ç®€åŒ–ç¤ºä¾‹
            throughput=len(data) / processing_time if processing_time > 0 else 0
        )
        
        final_results['processing_metrics'] = {
            'total_records': metrics.total_records,
            'processing_time_seconds': round(metrics.processing_time, 3),
            'throughput_records_per_second': round(metrics.throughput, 2),
            'batch_size': self.batch_size,
            'max_workers': self.max_workers
        }
        
        logger.info(f"å¤„ç†å®Œæˆï¼ç”¨æ—¶ {processing_time:.3f} ç§’")
        logger.info(f"ååé‡: {metrics.throughput:.2f} è®°å½•/ç§’")
        
        return final_results

def simulate_old_processing_time(num_records: int) -> float:
    """æ¨¡æ‹Ÿæ—§ç‰ˆæœ¬çš„å¤„ç†æ—¶é—´ï¼ˆç”¨äºå¯¹æ¯”ï¼‰"""
    # åŸºäºè®°å½•æ•°é‡çš„çº¿æ€§æ—¶é—´å¤æ‚åº¦ï¼ˆæ—§ç‰ˆæœ¬ï¼‰
    base_time = 0.001  # æ¯æ¡è®°å½•1æ¯«ç§’
    overhead = 0.5     # å›ºå®šå¼€é”€
    return num_records * base_time + overhead

async def performance_comparison_demo():
    """æ€§èƒ½å¯¹æ¯”æ¼”ç¤º"""
    print("=" * 60)
    print("èƒ½æºæ•°æ®å¤„ç†ç®¡é“æ€§èƒ½ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # æµ‹è¯•ä¸åŒæ•°æ®é‡
    test_sizes = [1000, 5000, 10000, 20000]
    
    pipeline = OptimizedEnergyPipeline(batch_size=1000, max_workers=4)
    
    for size in test_sizes:
        print(f"\nğŸ“Š æµ‹è¯•æ•°æ®é‡: {size:,} æ¡è®°å½•")
        print("-" * 40)
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_data = pipeline.generate_sample_data(size)
        
        # å¤„ç†æ•°æ®
        results = await pipeline.process_energy_data(test_data)
        
        # æ¨¡æ‹Ÿæ—§ç‰ˆæœ¬å¤„ç†æ—¶é—´
        old_time = simulate_old_processing_time(size)
        new_time = results['processing_metrics']['processing_time_seconds']
        
        # è®¡ç®—æ€§èƒ½æ”¹è¿›
        improvement = pipeline.calculate_performance_improvement(old_time, new_time)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"âœ… æ€»æ¶ˆè€—é‡: {results['total_consumption']:,.2f} kWh")
        print(f"âœ… å¹³å‡æ•ˆç‡: {results['avg_efficiency']:.2%}")
        print(f"âœ… æ€»æˆæœ¬: ${results['total_cost']:,.2f}")
        print(f"âœ… è®¾å¤‡æ•°é‡: {results['device_count']}")
        print(f"âœ… å³°å€¼æ¶ˆè€—: {results['peak_consumption']:,.2f} kWh")
        print(f"âœ… å•ä½æˆæœ¬: ${results['cost_per_kwh']:.4f}/kWh")
        
        print(f"\nğŸš€ æ€§èƒ½æŒ‡æ ‡:")
        print(f"   æ—§ç‰ˆæœ¬å¤„ç†æ—¶é—´: {old_time:.3f} ç§’")
        print(f"   æ–°ç‰ˆæœ¬å¤„ç†æ—¶é—´: {new_time:.3f} ç§’")
        print(f"   æ€§èƒ½æå‡: {improvement['improvement_percentage']:.1f}%")
        print(f"   åŠ é€Ÿå€æ•°: {improvement['speedup_factor']:.1f}x")
        print(f"   ååé‡: {results['processing_metrics']['throughput_records_per_second']:,.0f} è®°å½•/ç§’")

if __name__ == "__main__":
    # è¿è¡Œæ€§èƒ½å¯¹æ¯”æ¼”ç¤º
    asyncio.run(performance_comparison_demo())